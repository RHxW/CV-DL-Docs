# Inductive Representation Learning on Large Graphs(GraphSAGE)

> GraphSAGE的核心：GraphSAGE不是试图学习一个图上所有node的embedding，而是学习一个为每个node产生embedding的映射。

## Abstract
大规模图的节点低维嵌入法应用广泛，效果也好。但是目前大多数实现方法都需要图中所有节点参与训练；这些方法本质上是直推式(transductive)的而且无法直接泛化到没见过的节点。本文提出GraphSAGE，一个一般的归纳式框架，利用节点特征信息生成预先未见过的节点嵌入。与为每个节点训练其独立的嵌入不同，我们学习一个函数通过采样和聚合一个节点的局部邻居节点的特征来生成嵌入。

## 1 Introduction
大规模图节点的低维嵌入向量在很多任务中都可以作为很有用的输入特征来使用。节点嵌入法背后的思想是利用降维技术对图中节点的邻居的高维信息进行蒸馏得到一个稠密的向量。然后可以将这些节点向量送入后续的机器学习系统来完成不同的任务，比如节点分类、聚类和连接预测。

但是之前的工作都关注于一个单一固定图的节点嵌入，而很多现实任务都需要快速生成没见过节点或者整个新（子）图的嵌入。对于高吞吐的生产环境机器学习系统而言，这一归纳能力是必须的，因为它需要在不断进化的图上进行操作，而且会一直遇到没有见过的节点。一个归纳的生成节点嵌入的方法也可以促进拥有同样形式特征的不同图间的泛化能力：例如可以在一个模型有机体上训练一个蛋白质-蛋白质相互作用图嵌入生成器，然后迁移到一个新的有机体上利用现有模型生成数据。

与直推式的设定相比，归纳式节点嵌入问题格外困难，因为泛化到没见过的节点需要将新观察的子图与算法已经优化郭的进行“对齐”。一个归纳式框架必须学习识别一个节点的邻居的结构属性，这样的结构属性可以揭示节点在图中的局部角色和它的全局位置。

大多数现有方法都采用直推式生成节点嵌入。其中大多数方法通过使用基于矩阵分解的目标来直接优化每个节点的嵌入，因为它们在单一的固定图上进行预测，所以不能直接泛化到没见过的数据。这种方法可以通过修改变成归纳式方法，但是修改会增加计算复杂度，生成新预测前需要额外的梯度下降轮数。GCNs同样是直推式的方法，它也做用于固定图。本作干了两件事，一是将GCNs扩展到归纳式无监督学习，另一个是提出一个框架，将GCNs泛化来使用可训练的聚合函数（超越简单卷积）。

**当前工作** 
提出一个用于归纳式节点嵌入的通用框架，称为GraphSAGE(SAmple and aggreGatE).与基于矩阵分解的嵌入方法不同，我们利用节点特征（如文本属性、节点描述信息、节点度）来学习一个能够泛化到没见过节点的嵌入函数。通过在学习算法中结合节点特征，我们同时学习每个节点邻居的拓扑结构，以及节点特征在邻居中的分布。虽然我们关注特征丰富的图（例如带文本信息的引用数据，带公式/分子式标记的生物数据），但是我们的方法同样能够利用整个图的结构特征（如节点的度）。因此我们的算法同样可以应用于没有节点特征的图（？？？）。

我们不是针对每个节点训练一个明确的嵌入向量，而是训练一组聚合器函数，让它们学习去从一个结点的邻居中聚合特征信息(Figure 1).每个聚合器函数都从给定节点的不同跳数，或称为搜索深度处聚合信息。测试或推理的时候，使用训练好的系统，在全部没见过节点上应用学习后的聚合函数来生成它们的嵌入。根据之前生成节点嵌入的工作，设计一个无监督损失函数，它可以允许GraphSAGE脱离特定任务监督而训练。同时GraphSAGE也可以通过全监督的方式来训练。

![Figure 1](1.png "Figure 1")

在三个节点分类的基准上进行了测试，blahblahblah

## 2 Related work
**Factorization-based embedding approaches.** 一些最近的节点嵌入方法通过使用随机游走策略和基于矩阵分解的学习目标来学习低维嵌入。这些这些方法也和其他的一些经典算法联系紧密，例如谱聚类，多维缩放，以及PageRank算法。由于这些嵌入算法是针对独立的节点直接训练节点嵌入，因此它们本质上是直推式的，而且至少需要通过大量的额外训练才能在新节点上进行预测。而且其中很多方法的目标函数对于嵌入的正交变化是固定不变的，也就意味着嵌入空间无法在不同图之间自然泛化，还会在重训练的时候发生漂移。

**Supervised learning over graphs.** 除了节点嵌入法之外，有丰富的关于图结构数据的有监督学习的资料。其中包含多种基于核的方法，其图的特征向量来源于多种图核。最近还有一些神经网络的有监督学习方法。上述这些方法尝试对整个图进行分类，而我们的方法则关注于对每个结点生成有用的表达。

**Graph convolutional networks.** 近年提出了作用于图上的一些卷积神经网络架构。其中大部分无法扩展到大规模图或只设计用于整个图的分类。我们的方法与GCN的关系很紧密。原始的GCN算法的设计是用于直推式半监督学习的，具体的算法在训练的时候需要整个图的Laplacian矩阵。我们算法的一个简单的变体可以视为GCN框架在归纳式学习的一个扩展，具体见3.3

## 3 Proposed method: GraphSAGE
核心思想是学习如何从一个结点的邻居聚合特征信息（如附近节点的度或文本属性）。首先描述GraphSAGE的嵌入生成（即前向传播）算法。然后描述GraphSAGE模型的参数是如何使用SGD和反向传播技术进行学习的。

### 3.1 Embedding generation(i.e.,forward propagation) algorithm
本节描述生成嵌入的过程，或称前向传播算法(Algorithm 1)，这一过程假设模型训练完毕且参数固定。实际上我们假设学习了$K$个聚合函数（表示为$\text{AGGREGATE}_k, \forall k \in \{1,...,K\}$）的参数，这些聚合函数从节点的邻居处聚合信息，还有一组权重矩阵$\textbf{W}^k,\forall k\in \{1,...,K\}$，用于在模型的不同层（或“搜索深度”）间传播信息。

![Algorithm 1](A1.png "Algorithm 1")

```
|在每一层(K):
|    |对每一节点(V):
|    |    |1. 当前节点邻居在这一层的特征：由这些邻居在前一层的特征聚合得到
|    |    |2. 当前节点在这一层的特征：由当前节点在前一层的特征和它的邻居在这一层的特征拼接，点乘权重矩阵并激活得到
|    |对每一层的节点特征进行归一化
|
|得到节点的向量表示
```

Algorithm 1的直观理解是对于每一次循环或搜索深度，节点都会从它们各自的邻居处聚合信息，随着这一过程的继续，节点逐渐地获得越来越多来自图上更远处的信息。

Algorithm 1描述了在整个图$\mathcal{G=(V,E)}$以及每个结点的特征$\textbf{x}_v,\forall v \in \mathcal{V}$都作为输入的情况下的嵌入的生成过程。下面描述如何将这一过程推广到minibatch. Algorithm 1中的外层循环的过程如下，其中$k$代表外层循环的当前步（或称搜索深度），$\textbf{h}^k$代表一个节点在这一步中的表达：首先，每个节点将它*直接邻居*的表达$\{\textbf{h}_{u}^{k-1}, \forall u \in \mathcal{N}(v)\}$聚合成一个向量$\textbf{h}_{\mathcal{N}(v)}^{k-1}$.注意到这一聚合过程依赖于前一个循环中生成的表达（即$k-1$），而$k=0$（基础）的表达定义为输入结点特征。聚合完相邻的节点特征之后，GraphSAGE将节点的当前表达$\textbf{h}_v^{k-1}$和聚合得到的邻居向量$\textbf{h}_{\mathcal{N}(v)}^{k-1}$进行拼接，然后把这个拼接得到的向量传入一个带非线性激活函数$\sigma$的全连接层，转换成算法下一步会用到的表示（即$\{\textbf{h}_{v}^{k}, \forall v \in \mathcal{V}\}$）。为了方便表示，将第$K$层最终输出的表达表示成$\textbf{z}_v=\textbf{h}_v^K, \forall v \in \mathcal{V}$. 其中聚合的操作可以使用多种聚合器结构实现。

要将Algorithm 1扩展到minibatch上使用，在内部循环中只计算必要的节点而非全部节点(Appendix A)

**Relation to the Weisfeiler-Lehman Isomorphism Test.** GraphSAGE算法概念上受到了一些用于检测图同构性的经典算法的启发。在Algorithm 1中，如果：
(i) 设定$K=|\mathcal{V}|$,
(ii) 将权重矩阵设定为单位阵，
(iii) 使用一个合适的hash函数作为（无非线性）聚合器，
那么Algorithm 1就是一个Weisferler-Lehman(WL)同构测试的实例，aka “naive vertex refinement”. 如果Algorithm 1对于两个子图输出的表达集合$\{\textbf{z}_v, \forall v \in \mathcal{V}\}$是一样的，就说明这个两个子图经过WL测试的结果为二者同构。GraphSAGE是WL测试的一个连续近似实现，其中将hash函数替换成可训练的神经网络聚合器。当然我们使用GraphSAGE是为了生成有用的节点表达，而非测试图的同构性。尽管如此，GraphSAGE与经典的WL测试之间的联系仍为我们学习节点邻居拓扑结构的的算法设计提供了理论支撑。

**Neighbourhood definition.** 为了保证每个batch的computational footprint固定，我们在Algorithm 1中均匀采样一个固定尺寸的邻居集合，而非使用整个邻居集合。也就是在Algorithm 1中，将$\mathcal{N}(v)$定义为固定尺寸、从集合$\{u \in \mathcal{V}:(u,v) \in \mathcal{E}\}$中均匀采样得到的节点集合，并且在每个循环$k$中都进行均匀采样。如果不这样采样的话，一个单独batch占用的内存和预期运行时间都无法预测，最差可能达到$O{|\mathcal{V}|}$. 作为对比，GraphSAGE每个batch的空间和时间复杂度是固定的，为$O(\Pi_{i=1}^K S_i)$，其中$S_i,i\in \{1,...,K\}$，$K$是指定的常数。实际上我们发现在$K=2 \quad and \quad S_1 \cdot S_2 \le 500$的时候效果更好。

### 3.2 Learning the parameters of GraphSAGE
为了能够无监督地学习有用的、可以预测的表示，我们对于输出的表达$\textbf{z}_u, \forall u \in \mathcal{V}$采用了一种基于图的损失函数，并且通过SGD来调整更新权重矩阵$\textbf{W}^k, \forall k \in \{1,...,K\}$和聚合器函数的参数。基于图的loss函数会推动相邻的节点获取相似的表达，并强制使不相关的节点表达呈现较大差异：
$$
J_{\mathcal{G}}(\textbf{z}_u)=-\log(\sigma(\textbf{z}_u^T\textbf{z}_v))-Q \cdot \mathbb{E}_{v_n \sim P_n(v)}\log(\sigma(\textbf{z}_u^T\textbf{z}_{v_n})), \qquad (1)
$$
其中$v$是从$u$进行固定长度随机游走时出现的节点，$\sigma$是sigmoid函数，$P_n$是一个负（样本）采样分布，$Q$定义了负样本的个数。重要的是，与之前的embedding方法不同，我们输入这个损失函数的表示$\textbf{z}_u$是通过一个节点的局部邻居所包含的特征生成得到的，而非对每一个节点单独训练得到一个embedding.

这一无监督的设定模拟了提供给下游机器学习应用的节点特征作为一个服务或者一个固定仓库的情况。如果表达用于某一特定的下游任务的话，可以将上面提到的无监督损失函数（公式1）替换成与特定任务相关的目标函数（如交叉熵）。

### 3.3 Aggregator Architectures
与在N-D的网格上进行的机器学习不同，一个节点的邻居本身是无序的；因此Algorithm 1中的聚合器函数必须在一个无需的向量集合上进行操作。理想状况下，一个聚合器函数应该是对称的（即对输入顺序的变化具有不变性），同时还需要是可训练且有较高表达能力。聚合函数对称的特点保证了神经网络模型能够在任意顺序排列的节点集合上进行训练和应用。我们检查了三个聚合器函数：

**Mean aggregator.** 第一个可选的聚合器函数是平均算子，实现起来就是使用向量$\{\textbf{h}_u^{k-1}, \forall u \in \mathcal{N}(v)\}$的元素级平均。平均聚合器基本上就相当于直推式GCN框架中的卷积传播法则。实际上如果将Algorithm 1中第4，5行改为下面的公式，就可以得到一个归纳式的GCN变体：
$$
\textbf{h}_v^k \leftarrow \sigma(\textbf{W}\cdot \text{MEAN}(\{\textbf{h}_v^{k-1}\} \cup \{\textbf{h}_u^{k-1}, \forall u \in \mathcal{N}(v)\})). \qquad (2)
$$
因为这个基于平均修改得来的聚合器基本上可以算是局部谱卷积的一个粗略的线性近似，所以将这个聚合器称作*卷积*。这个卷积聚合器和其他聚合器的一个很重要的不同在于它不会进行Algorithm 1中第五行的拼接操作，即这个卷积聚合器会将节点在之前层的表达$\textbf{h}_v^{k-1}$与聚合后的邻居向量$\textbf{h}_{\mathcal{N}(v)}^k$进行拼接。这一拼接操作可以视为在不同“搜索深度”或“层”间的简单版的“跳跃链接”，这实现了较大的性能增益。
>注：这里的mean aggregator就如作者所说，与gcn没有什么区别，因为在gcn中有$H^{(l+1)}=\sigma \big(\hat{A}H^{(l)}W^{(l)}\big)$,这个式子中**归一化的**邻接矩阵和上一层的特征相乘的操作基本上就相当于GraphSAGE中提到的mean aggregator，不过gcn是根据A进行整体的加权平均，而GraphSAGE是在邻域内进行平均，所以GraphSAGE天生支持batch操作

**LSTM aggregator.** 同样尝试了基于LSTM架构的更复杂的聚合器。与平均聚合器相比，LSTMs的优势在于更大的表达容量。但是需要注意的是由于LSTMs按序列形式处理数据，所以它们本身并不是对称的（即它们对于顺序是敏感的）。通过在随机顺序的节点邻居上使用LSTMs来将LSTMs修改为能在无序集合上使用的形式。

**Pooling aggregator.** 我们尝试的最后一个聚合器即对称又可训练。在这个*池化*的方法中，每个邻居的向量都独立地传入一个全连接神经网络；在这一变换之后，采用一个最大池化操作来聚合跨邻居集合的信息：
$$
\text{AGGREGATE}_k^{\text{pool}}=\max(\{\sigma(\textbf{W}_{\text{pool}}\textbf{h}_{u_i}^k+\textbf{b}),\forall u_i \in \mathcal{N}(v)\}), \qquad (3)
$$
其中max代表元素级max算子，$\sigma$是一个非线性激活函数。原则上，在max操作之前使用的函数可以是一个任意的深度多层感知器，但是本文中我们只关注简单的单层架构。直观上这个多层感知器可以看作一组函数的集合，这些函数对每个节点在邻居集合中的表达进行计算得到特征。通过在每个计算得到的特征上使用最大池化算子，模型有效地捕捉到了邻居集合的不同方面。同样值得注意的是，原则上任何对称的向量函数都可以替换max算子（如元素级平均）。在测试中没发现max-pooling和mean-pooling有太大差距，后续都使用max-pooling.



## Appendices
### A Minibatch pseudocode
在GraphSAGE的前向传播中，minibatch$\mathcal{B}$包含了要生成表达的节点。
![Algorithm 2](A2.png "Algorithm 2")

中心思想是首先对计算用到的全部节点进行采样。Algorithm 2中的2-7行对应采样阶段。每一个集合$\mathcal{B}^k$都包含计算节点$v\in \mathcal{B}^{k+1}$的表示所需的全部节点，即Algorithm 1中第$(k+1)$个循环，或“层”的概念。第9-15行对应聚合阶段，与batch推理算法几乎一致。注意到第12和13行，第$k$次循环中$\mathcal{B}^k$的所有节点的表达都可以计算得到，因为它们以及采样的邻居在$k-1$次循环中的表达已经在前次循环中计算完毕了。因此，算法也就避免计算不在当前minibatch中或者没有在本次循环里用到的节点。使用符号$\mathcal{N}_k(u)$来表示一个确定的函数，它指明了一个节点邻居的一个随机样本（即假设随机性以预先计算好）。对函数采用k个索引表示其随机样本在k个循环是独立的。本文使用均匀采样函数并使用有放回策略以免节点的度小于采样数量。

注意到Algorithm 2的采样过程在概念上与Algorithm 1是相反的：从"layer-K"的想要生成表达的节点开始（即$\mathcal{B}$中的节点）；然后采样它们的邻居（即算法中"layer-K-1"的节点）然后继续。这样做的一个后果就是邻居采样的规模可能是反直觉的。实际上如果使用总迭代$K=2$以及$S_1,S_2$，也就代表在$k=1$的循环中采样$S_1$个节点，在$k=2$的循环中采样$S_2$个节点——从$\mathcal{B}$中我们想要在迭代$k=2$之后生成表示的“目标”节点来看——这代表着它们直接邻居的采样数$S_2$，以及2跳邻居采样数$S_1\cdot S_2$.