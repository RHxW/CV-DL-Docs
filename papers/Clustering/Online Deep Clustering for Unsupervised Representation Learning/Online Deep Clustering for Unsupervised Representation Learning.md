# Online Deep Clustering for Unsupervised Representation Learning

## Abstract

联合聚类和特征学习方法在无监督表示学习中展现出了强大的能力。但是，其训练计划在特征聚类和更新网络参数之间的切换会导致视觉表达的不稳定的学习。为了解决这一问题，提出在线深度聚类(ODC, Online Deep Clustering)，同时进行聚类和更新网络的操作。关键思想是聚类中心应该在分类器稳定更新的时候保持稳定。特别地，我们设计了两个动态存储器模块，即用于存储样本的标签和特征的采样存储器模块和用于中心演变的中心存储器模块。将粗暴的全局聚类拆分成稳定的存储器更新和按batch的标签重分配。这个流程整合到了网络更新的迭代中。

![Figure 1](1.png "Figure 1")

## 1. Introduction

无监督表示学习的目标是在不许手动标注的条件下学习到图像或者视频的可转移表达。其中基于聚类的表示学习方法成为了这一领域有前景的方向。与基于复原的方法不同，基于聚类的方法只需要少量的域只是就可以达到较好的效果。与对比表达学习只捕获图片内不变性相比，基于聚类的方法能够探索图片间的相似度。与通常作用与固定特征之上的传统聚类相比，这些工作同时进行聚类和特征学习。

尽管早期作品几乎都在小数据集上进行验证，Caron et al.提出的Deep Clustering(DC)第一个尝试提升基于聚类的表达学习的规模。DC在特征聚类和CNN参数更新之间切换。特别地，在每个epoch开始的时候，DC在整个数据集上执行离线聚类算法来获得伪标签作为下一个epoch的监督信号。离线聚类必然会改变不同epochs的标签，也就是说就算有些聚类不发生变化，它们的索引也会在聚类后被随机修改。结果就是当前分类器的参数无法继承到下一个epoch，因此每个epoch开始时分类器的参数就得进行随机初始化。这样的机制引入了训练的不稳定性并将表达暴露在了representation corruption的风险下。如Figure 1(a)所示，每个epoch中，DC的网络更新被特征提取和聚类打断了。与此相对，传统的有监督分类，采用使用固定标签的无中断方式，其每次迭代都由网络的前向和反向传播组成。

本作中想要设计一个高稳定性的联合聚类和特征学习的范式。为了减少训练机制在DC和监督学习间的矛盾，将聚类进程分解为mini-batch级的标签更新，并将这一更新过程融合到网络更新的迭代中。基于这一直觉，提出用于联合聚类和特征学习的在线深度聚类（ODC）。特别地，ODC的一个迭代由前向和反向传播、标签重分配和中心更新组成，因此就避免了额外的特征提取操作。为了帮助在线标签重分配和中心更新，我们设计并维护了两个动态存储器模块，即用于存储样本的标签和特征的采样存储器模块和用于中心演变的中心存储器模块。以这种方式，ODC就可以像有监督分类一样用无中断的方式训练，且不需要手动标注信息。在训练过程中，标签和网络参数同时（而非分别）变化。由于标签是在每次迭代中连续且立刻更新的，CNN中的分类器也同样保持稳定，会得到一个更稳定的loss曲线，如Figure 1(b)所示。

尽管只使用ODC就能达到很好的无监督表示学习效果，同时它也可以用来对使用其他无监督方法训练的模型进行微调。



## 2. Related Work

blahblahblah



## 3. Methodology

### 3.1. Online Deep Clustering

首先探讨一下DC的基本理念，然后展开ODC的细节。为了学习表达，DC需要在离线特征聚类和使用伪标签的网络反向传播之间进行切换。这个离线聚类过程需要在整个训练集上提取深度特征，使用一个全局聚类算法，例如K-Means聚类。全局聚类算法在很大程度上替换了伪标签，需要网络在随后的epoch中迅速适应新标签。

![Figure 2](2.png "Figure 2")

> ODC的每次迭代主要包含四步：1. 前向传播获取特征向量；2. 从样本存储器中读取标签并实施反向传播更新CNN参数；3. 通过更新特征来更新样本存储器并分配新标签；4. 通过重新计算涉及的类中心来更新类中心存储器

**Framework Overview. ** 与DC不同，ODC不需要额外的特征提取过程。另外，标签的更替和网络参数的更新同步进行。这一可能是通过新引进的样本存储器和中心存储器实现的。如Figure 2所示，样本存储器存储整个数据集的特征和伪标签；中心存储器存储类别中心的特征，即一个类别中所有样本的平均特征。这里，一个“类别”是指在训练过程中不断进化的一个临时的聚类。在ODC的每次无中断迭代中，标签和网络参数是同时更新的。引入包括loss重加权和处理小聚类等方法来避免ODC陷入平凡解。

**An ODC Iteration. ** 假设给定一个随机初始化的网络$f_{\theta}(*)$和一个线性分类器$g_{w}(*)$，目标是训练backbone的参数来生成具有高度分辨能力的表达。为ODC准备的样本和中心存储器通过一个全局聚类过程进行初始化，如K-Means. 接下来就可以无中断地进行ODC迭代了。

一个ODC的迭代中包含四个步骤。

* 首先，给定一个batch的输入图片${x}$，网络将图片映射到紧凑的特征向量$F=f_{\theta}(x)$. 
* 第二步，从样本存储器中读入这个batch的伪标签。使用伪标签来更新网络，用随机梯度下降来解决下述问题：

$$
\min_{\theta,w} \frac{1}{B}\sum\limits_{n=1}^B l(g_w(f_{\theta}(x_n)),y_n), \qquad(1)
$$

其中$y_n$是样本存储器中的当前伪标签，$B$代表每个mini-batch的大小。

* 第三步，使用经过L2正则化的$f_{\theta}(x)$来更新样本存储器：

$$
F_m(x)\larr m\frac{f_{\theta}(x)}{\Vert f_{\theta}(x) \Vert _2}+(1-m)F_m(x), \qquad (2)
$$

其中$F_m(x)$是$x$在样本存储器中的特征，$m\in(0,1]$是一个动量系数。其中的每个样本都会通过下述的寻找最近类中心的方式，被同步地分配一个新标签：
$$
\min_{y\in \{1,...,C\}} \Vert F_m(x)-C_y \Vert_2^2, \qquad (3)
$$
其中$C_y$代表类别$y$的中心点。

* 最后，每个涉及到的中心，包括那些新成员加入的，和老成员留下的，都会被记录下来。这些中心每第$k$个循环通过对所有对应类别的样本的特征进行平均得到。

### 3.2. Handling Clustering Distribution in ODC

**Loss Re-weighting. ** 为了避免训练坍塌到几个巨大的聚类，DC在每个epoch前都采取了均匀采样。但是对于ODC而言，聚类样本的数量每个循环都在变。使用均匀采样需要在每个循环对整个数据集进行重新采样，这个过程被视为是冗余且消耗巨大。提出一个替换的方法，即根据每个类别中样本数量对loss进行重新加权。为了验证二者的等效性，使用重加权实现了一个DC模型，从经验上发现当权重$w_c \varpropto \frac{1}{\sqrt{N_c}}$时（其中$N_c$代表类别$c$中的样本数量），性能不变。因此，ODC也采用了相同的loss重加权公式。使用loss重加权，较小的聚类中的样本在反向传播中的贡献更大，也就推动了决策边界进一步接受更多的潜在样本。

**Dealing with Small Clusters. ** Loss重加权帮助阻止了巨型聚类的形成。然而我们仍然面对一些小聚类坍塌成空聚类的风险。为了解决这一问题，提出在这些特别小的聚类坍塌之前就预先处理并消灭它们的方法。用$\mathcal{C}_n$代表规模大于一个阈值的正常聚类，用$\mathcal{C}_s$代表规模没超过这个阈值的小聚类，对$c\in \mathcal{C}_s$，首先将$c$中的样本都分给$\mathcal{C}_n$中与其距离最近的中心，使$c$变成空聚类。然后将最大的聚类$c_{max}\in \mathcal{C}_n$使用K-Means分成两个子聚类，并随机选一个作为新的$c$. 重复这个过程知道所有的聚类都属于$\mathcal{C}_n$. 尽管这一过程粗暴地改变了一些聚类，但它仅影响少量的样本。

**Dimensionality Reduction. ** 有些backbone网络将一张图片映射为一个高维向量，如AlexNet产生4096维的特征，ResNet-50获取2048维特征，这就导致后续的聚类需要高空间和时间复杂度。DC在整个数据集上使用PCA来降维。但是对于ODC，不同样本的特征的时间戳是不同的，导致样本间的统计信息不具可比性。因此PCA就不再适用了。每次迭代都使用PCA同样成本很高。因此加入一个非线性head层{fc-bn-relu-dropout-fc-relu}来将高维特征降低为256维。它在ODC迭代中一起训练。这个head层在下游任务(downstream tasks)中移除。



### 3.3. ODC for Unsupervised Fine-tuning

与自监督学习方法往往捕捉图像内语义相比，基于聚类的方法更多地关注图像间的信息。因此，DC和ODC天生与前序的自监督学习方法互补。由于DC和ODC并不受限于特定的目的，如旋转角或颜色的预测，它们可用作无监督的微调方法来提升已有的自监督方法的性能。本文研究DC和ODC作为由不同自监督学习方法初始化的微调过程的效果。



### 3.4. Implementation Details

**Data Pre-processing. ** 将ImageNet的无标签的128万张图片用作训练。首先随机裁剪到分辨率为224*224并进行包括随即反转和旋转(±2°)在内的增广操作。DC使用Sobel滤波器来避免利用颜色作为捷径。这个预处理步骤需要下游任务同样包含Sobel层，潜在地限制了其应用。我们发现强颜色抖动表现出了与Sobel滤波在避免捷径上的相同的效果，而且抖动允许接收普通的RGB图像作为输入。（采用了一系列数据增广的方式）这样可以阻止网络利用颜色提供的平凡信息。

**Training of ODC. ** 使用ResNet-50作为backbone. 