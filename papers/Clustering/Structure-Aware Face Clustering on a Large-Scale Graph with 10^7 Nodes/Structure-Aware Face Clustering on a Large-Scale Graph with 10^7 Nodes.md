# Structure-Aware Face Clustering on a Large-Scale Graph with 10^7 Nodes

## Abstract

STructure-AwaRe Face Clustering（STAR-FC）方法，设计了一个能够保留结构信息的子图采样策略，用于探索大规模训练数据的能量，可将训练数据的尺度从1e5提升至1e7. 推理的时候STAR-FC在整个图上进行聚类，两个步骤：图解析(graph parsing)和图微调(graph refinement). 第二步引入节点亲密度(node intimacy)来挖掘局部结构信息。

![Figure 1](1.png"Figure 1")

## 1. Introduction

基于GCN的监督学习聚类方法是通过邻接图来实现的，一般可以分为两类：局部方法和全局方法，区别在于GCN的输入是否是整个图。

提出一个结构感知人脸聚类方法STAR-FC来解决大规模训练和高效推理的困境。特别地设计一个基于knn图的GCN来估计边的置信度。进一步提出保留结构的子图采样策略用于进行大规模GCN训练。推理的时候，分两步继续宁人脸聚类：图解析和图微调。第二步中，引入节点亲密度来挖掘局部结构信息用于后续微调。在推理过程，整张图都会作为输入以提升效果。

实验表明，在这些结构感知设计的帮助下，STAR-FC不仅可以实现基于采样的训练，还可以实现全图推理。基于采样的训练中，训练数据能够提升两个数量级，从1e5到1e7.甚至更高。



## 2. Related Work

blahblahblah



## 3. Methodology

### 3.1. Overview

STAF-FC的总体结构见Figure 2

![Figure 2](2.png"Figure 2")

训练的时候通过保留结构的子图采样策略来训练基于GCN的边置信度估计器。目标是通过采样的子图来近似整个图的结构，并保留大多数对训练贡献较大的困难负样本（边）。通过这种方式就可以释放大规模数据的潜力。我们将边的预测建模成一个二分类问题，并使用交叉熵损失作为监督。推理的时候，将人脸聚类分为两步：图解析和图微调。图解析是将整个图作为输入来估计全部边的置信度得分。然后将得分较低的边从图中移除，此时图的结构变得更清晰。但是其中仍然存在错误的连接。这些错误的链接评分相对较高，因此难以去除。为了对图做进一步的调整，引入节点亲密度进行再次剪枝。这两步完成之后，人脸聚类族就自然形成了。

### 3.2. Large-Scale GCN Training

**Design of the GCN. ** 这一步设计一个基于GCN的，在knn邻接图上使用的边置信度预测器。首先使用一个训练好的ResNet-50获得特征矩阵$F\in \mathbb{R}^{N\times D}$，其中$N$是人脸图片的数量，$D$是特征的维度。为了构建knn邻接图，每个样本都当作图中的一个节点，并连接到$K$个最近的相邻节点。对应的稀疏对称邻接矩阵是$A \in \mathbb{R}^{N\times N}$.

因为CNN通过强监督进行训练，它提取的特征$F$实际上就包含了丰富的身份信息。但是由于类内差异和knn固定的$K$值，邻接图可能会包含很多错误的边。因此使用GCN进相邻信息的传播，尝试直接预测边是否应该保留。使用的GCN骨干网络，一个$L$层的计算过程可以表示为：
$$
F_{l+1}=\sigma \Big( [F_l^T,(\tilde{A}F_l)^T]^TW_l \Big), \qquad(1)
$$
其中$\tilde{A}=\tilde{D}^{-1}(A+I)$. $\tilde{D}$是对角的度矩阵，$\tilde{D}=\sum_j\tilde{A}_{ij}$. $F_l$代表第$l$层的embeddings，$F_0$是输入的人脸特征。$W_l \in \mathbb{R}^{D_{in}\times D_{out}}$是可学习的矩阵，将embeddings映射到一个新的空间。$\sigma$是一个非线性激活函数，使用的是ReLU. $F_L$代表$L$层的输出。

由于$F_L$将相邻节点的很多信息聚合到一起并对图的结构信息进行编码，这使它更适合人脸聚类任务。为了预测边是否保留，设计了一个2层MLP的二分类网络，使用边置信度和gt计算交叉熵loss. 具体操作是将一条边所连接的一对特征作为MLP的输入得到边置信度的预测值。边的gtlabel是0或1

这样就可以通过一个阈值$\tau_1$来去掉错误的边。但是这样可能导致两种误判：1）可能去掉了少数的正确边；2）可能保留了少数的错误边；由于原始的邻接图的连接很稠密，所以丢失正确边对结果的影响很小。

**Structure-Preserved Subgraph Sampling. ** GCN的训练中，全局方法显存跟不上，局部方法缓解了显存的负担但同时严重依赖于重叠的子图，因此在效率和精度上都受到严重的影响。

为了充分利用大规模数据集，设计了一个保留结构的子图采样(SPSS, structure-preserved subgraph sampling)策略用来训练GCN. 一个邻接图中的边主要由两部分组成