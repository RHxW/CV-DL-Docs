# MagFace
## Abstract
人脸识别系统的性能随着人脸多样性的增加而下降。先前的办法要么预先对人脸质量做处理，要么与人脸特征同时预测数据不确定性。本文提出一类loss称为MagFace，用于学习一个全局特征embedding，其大小可以用来衡量给定人脸的质量。在新loss下，可以证明特征embedding的大小与目标被成功识别的可能性呈正相关关系。而且，MagFace引入一种自适应机制来学习一种结构良好的类内特征分布，方法是通过将简单样本拉向类中心，将困难样本推离类中心。这样做能够防止模型对噪音低质量图片过拟合并且可以提升非限制场景人脸识别效果。
![Figure 1](1.png 'Figure 1')

## 1. Introduction
最近的置信度感知方法用来将每个人脸便是为隐空间的一个高斯分布，分布的均值估计了最接近的特征值，同时方差表示特征值的不确定性。尽管性能有所提升，但是这些方法会将人脸特征学习和数据噪音建模分离开。因此要引入额外的网络部分来计算每个图片的不确定性。这样做让训练过程变复杂了而且在推理中增加了计算量。而且不确定性的度量无法用传统方式对人脸特征进行度量。
本文引入学习全局和质量感知的人脸表示的方法，MagFace. MagFace的设计遵循了两条宗旨：
1. 给定一个人不同质量的图片，它会去学习一个类内分布，使高质量人脸图片与类中心的距离近，而低质量图片则分布在类边缘
2. 在替换现有推理架构时的成本应该尽可能小

为了达到上述目标，选择数值大小，这个与特征向量方向无关的属性，作为质量评估的指标。MagFace的核心目标是在增大类间距离的同时保持一个圆锥形的类内结构，如Figure 1b所示，其中不明确样本会被从类中心推开。这是通过在训练的时候给不明确样本适应性低权重并且在loss中对数值较大的特征给予奖励实现的。

## 2. Related Works
### 2.1. Face Recognition
### 2.2. Face Quality Assessment
### 2.3. Face Clustering

## 3. Methodology
### 3.1. ArcFace Revisited
ArcFace的目标函数为：
$$
L=-\frac{1}{N}\sum_{i=1}^N \log \frac{e^{s\cos (\theta_{y_i}+m)}}{e^{s\cos (\theta_{y_i}+m)}+\sum_{j\neq y_i}e^{s\cos \theta_j}}
$$
其中$m>0$代表附加的角度边界，s是缩放参数
尽管ArcFace的效果很好，但是角度边界惩罚m与质量是没有关系的，而且得到的类内分布结构在非限制场景下可能是没有规律的。例如，如Figure 2a所示，有同一个人的三种质量等级的图片，用圆圈大小表示，圆圈越大代表人脸表示的不确定性越高、识别难度越大。因为ArcFace使用同一的边距值m，所以同一类别中的所有图片共享相同的决策边界，即关于相邻类别有$B:\cos(\theta +m)=\cos(\theta')$.这三种样本的所在位置在可能区域中是不确定的，见Figure 2a中的阴影区域，而角度边界无法对区域做任何惩罚。这会导致不稳定的类内分布，例如高质量人脸(1)更靠近边界B而更低质量的(2,3)更靠近类中心w. 这种不稳定性会影响到非配合场景的识别效果。而且，对于困难样本和噪声样本而言，由于它们难以停留在可能的区域，因此它们的权重过大，就导致模型可能对它们过拟合。
![Figure 2](2.png 'Figure 2d')
### 3.2. MagFace
根据上述分析，基于余弦相似度的人脸识别loss除了一个固定的边距m以外，缺乏更细粒度的限制。这导致类内结构的不稳定，尤其对于非限制情形，每个目标的人脸图片差异巨大。为了解决这一问题，提出MagFace，一个新的框架将质量度量编码进人脸表示中。我们的设计追求优化向量大小$a_i=\lVert f_i \rVert$而不对其进行归一化。这种设计有两个优点：
1. 可以继续使用基于余弦相似度的度量
2. 通过同时限制方向和大小，人脸表达会更鲁棒

在定义loss之前，先引入两个与$a_i$相关的辅助函数：大小感知的角度边距$m(a_i)$和正则化项$g(a_i)$.$m(a_i)$的设计遵循了一个很自然的直觉：高质量样本的确定性较高且应向较小聚类集中。假设大小和质量存在正相关关系，则对$x_i$的$m(a_i)$进行惩罚。为了更好地理解，Figure 2b可视化了不同向量长度的边界$m(a_i)$.与Figure 2a的ArcFace相比，可能区域变小了。结果，这样的边界将低质量样本（Figure 2c的2和3）拉向原点使它们有更低的被惩罚风险。但是仅由$m(a_i)$构建的结构对高质量样本是不稳定的，例如Figure 2c中的1，因为它们在可能区域中有更大的自由度。因此引入正则化项$g(a_i)$来奖励有更大长度向量的样本。通过将$g(a_i)$设计成一个关于$a_i$的单调递减凸函数，使每个样本都会被推向可能区域的边界，而高质量样本会更靠近类中心w，如Figure 2d所示。简而言之，MagFace用大小感知边界和正则化项来获得更高的类间分离度和更高的类内相似度，从而强化了ArcFace，其优化方式为：
$$
L_{Mag}=\frac{1}{N}\sum_{i=1}^N L_i \\
L_i=-\log \frac{e^{s\cos (\theta_{y_i}+m(a_i))}}{e^{s\cos (\theta_{y_i}+m(a_i))}+\sum_{j\neq y_i}e^{s\cos \theta_j}} + \lambda_g g(a_i)
$$
超参数$\lambda_g$用于在分类loss和正则化loss间做调整。
MagFace的设计不仅遵循直觉，同时也有理论支持。假设向量长度$a_i$的范围是$[l_a,u_a]$，其中$m(a_i)$是一个严格增的凸函数，$g(a_i)$是一个严格减的凸函数，而$\lambda_g$足够大，则可证明当针对$a_i$优化$L_i$的时候，MagFace的loss一直都有下列两个属性：
**Property of Convergence.** 对于$a_i\in[l_a,u_a],L_i$是一个严格凸函数，有最优解$a_i^*$
**Property of Monotonicity.** 最优解$a_i^*$随着样本与对应类中心的cosine距离的减小单调增加

收敛的性质保证了对$a_i$有唯一最优解，同时也保证了快速收敛。单调的性质说明特征的量级代表识别的难度，因此可以视作一种质量的度量

实际上，作者使用的m和g函数的具体形式为：
$$
m(a_i)=\frac{u_m-l_m}{u_a-l_a}(a_i-l_a)+l_m \\
g(a_i)=\frac{1}{a_i}+\frac{1}{u_a^2}a_i
$$
对函数g求导，可以发现，当$a_i$小于$u_a$的时候，函数单调减，大于$u_a$的时候，单调增。由此限制magnitude过大。
作者训练时将$\lambda_g,l_a,u_a$固定为35,10,110.然后调整$l_m,u_m$寻找最佳配置，结论是$l_m,u_m$设置为0.4,0.8的时候效果最好
### 3.3. Analysis of Feature Magnitude
