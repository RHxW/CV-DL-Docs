# Unsupervised Domain-Specific Deblurring via Disentangled Representations

图像去模糊的目的是从对应的模糊图像复原图像的隐含形状。本文提出一种基于解耦表示的，用于特定域的单张图像去模糊的无监督方法。通过使用内容编码器和模糊编码器分离一张模糊图像的内容和模糊特征，从而实现解耦。采用KL散度loss用于正则化提取的模糊属性的分布区间，从而保证只包含很少的内容信息。同时，为了解决不成对数据，加入模糊分支和循环一致性loss以确保去模糊结果的内容结构域原始图像吻合。还在去模糊的结果上加入一个对抗loss，用来生成视觉上真实的图片，加入一个感知（perceptual）loss，进一步削弱人工痕迹。

![Figure 1](1.png"Figure 1")

## 1. Introduction

为了解决图像模糊问题，blind image 去模糊的目标是从对应的模糊图像复原图像的隐含形状。大多数传统方法将图像去模糊任务视为一个模糊核估计问题。由于这一问题有严重的不适定性，提出了很多方法来对图像和核进行建模。但是大多数方法只在一般的自然图片上的表现尚可，对于特定域的图像，如人脸，文字核低光照图片等就无法泛化。于是出现了一些用于特定域图像去模糊的方法。但是这些方法仍然只能解决特定类型的模糊，而且一般需要较长推理时间。

最近一些基于学习的方法用于blind image 去模糊。基于CNN的模型可以解决更复杂的模糊类型而且其容量足够大，可以在大尺度数据集上进行训练。同时，GAN被发现可以生成更真实图片。但是这些方法都需要成对的训练数据，获取成本较高。尽管很多模糊生成的方法被开发出来，到那时它们并不能学习现实场景下所有类别的模糊变体。而且强监督会引起算法过拟合，从而无法泛化。

![Figure 2](2.png"Figure 2")

本文提出一种基于解耦表示的，无监督的特定域图像去模糊方法。具体来说，将模糊图像的内容和模糊特征解耦，以精准编码模糊信息到去模糊框架中。如Figure 2所示，内容编码器从未成对的清晰与模糊图片提取内容特征，模糊编码器提取模糊信息。两个编码器共享最后层的权重，这样内容编码器就可以将两个域的内容特征投影到一个一般空间。但是如果只使用这个架构，无法确保模糊编码器能够捕捉模糊特征，它也许会编码内容信息。加入一个KL散度loss用于正则化模糊特征的分布来压制其包含的内容信息。然后去模糊生成器$G_S$和模糊生成器$G_B$接收对应模糊属性下的内容特征用于生成去模糊图片和模糊图片。和CycleGan类似，同样使用对抗loss和循环一致性loss来帮助生成器网络获取更多显示图片，同时保留原始图片的内容。为了进一步去除生成器$G_S$引入的令人讨厌的人工痕迹，在框架中加入感知loss，一些去模糊后的样本见Figure 1



## 2. Related Works

### 2.1. Single Images Blind Deblurring

**Generic methods:** 单张图片blind deblurring是一个高度不适定问题。过去十年间，出现了一些自然图像和核先验的方法用来正则化隐含清晰图像的解空间，包括重尾梯度先验，稀疏核先验，$l_0$梯度先验，标准化稀疏性先验和暗通道法。但是这些先验建立的基础是有限的观察集，它们不够准确。就导致去模糊后的图像通常要么未完全去模糊（图片仍然模糊），要么过度去模糊（图片包含太多人为因素）。

另一方面，由于最近深度网络和GAN的巨大成功，一些基于CNN的方法被提出用于图片去模糊。

**Domain specific methods:** 尽管上述方法对自然图片的去模糊效果不错，却很难将它们泛化到一些特定图像域，例如人脸和文字图片。

### 2.2. Disentangled Representation

最近在解耦表示方向上有些努力。最近一些无监督方法将图像解耦成域不变内容特征和域特定属性向量，生成不同的图片到图片的变换输出。



## 3. Proposed Method

提出的框架由四部分组成：

1）内容编码器$E_B^c$和$E_S^c$用于对模糊和清晰图片域；

2）模糊编码器$E^b$

3）模糊和清晰图像生成器$G_B$和$G_S$

4）模糊和清晰图像判别器$D_B$和$D_S$

给定一个训练样本$b\in B$属于模糊图像域，和$s\in S$属于清晰图像域，内容编码器$E_B^c$和$E_S^c$从对应样本提起内容信息，$E^b$从$b$估计模糊信息。然后$G_S$使用$E_B^c(b)$和$E^b(b)$生成一张清晰图片$s_b$，同时$G_B$使用$E_S^c(s)$和$E^b(b)$生成一张模糊图像$b_s$. 判别器$D_B$和$D_S$区分真实样本和生成样本。端到端的架构如Figure 2所示

### 3.1. Disentanglement of Content and Blur

由于在非配对条件下无法获得gt清晰图片，从一张模糊图片种解耦出内容信息就很重要（非小事，not trivial）。但是由于清晰图片只包含内容组成而没有任何模糊信息，内容编码器$E_S^c$应该是一个好的内容提取器。我们强制编码器$E_B^c$和$E_S^c$共享最后一层的权重，目的是引导$E_B^c$学习如何从模糊图像种高效地提取内容信息。

另一方面，模糊编码器$E^b$应该只编码模糊信息。为了达成这一目的，提出两个方法来帮助$E^b$尽可能地一致内容信息。首先，将$E_S^c(s)$和$E^b(b)$传入$G_B$生成$b_s$. 由于$b_s$是$s$的模糊版本，其并不包含$b$的内容信息，这个结构阻止$E^b(b)$编码$b$的内容信息。其次，加入一个KL散度loss用于将模糊特征$z_b=E^b(b)$正则化为正态分布$p(z)\sim N(0,1)$. 这会进一步抑制$z_b$中包含的信息。KL散度loss定义为：
$$
KL(q(z_b)\Vert p(z))=-\int q(z_b)\log \frac{p(z)}{q(z_b)}dz \qquad(1)
$$
经过证明，最小化KL散度等价于最小化下述loss：
$$
\mathcal{L}_{KL}=\frac{1}{2}\sum_\limits{i=1}^{N}(\mu_i^2+\sigma_i^2-\log(\sigma_i^2)-1) \qquad (2)
$$
其中$\mu$和$\sigma$是$z_b$的均值和标准差，$N$是$z_b$的维度。$z_b$的采样方式为$z_b=\mu+z\circ\sigma$，其中$p(z)\sim N(0,1)$，$\circ$代表元素对应相乘。

### 3.2. Adversarial Loss

为了使生成的图像看起来更真实，我们在两个域上都使用了对抗损失。对清晰图片域，其对抗损失定义为：
$$
\mathcal{L}_{D_S}=\mathbb{E}_{s\sim p(s)}[\log D_S(s)]+\mathbb{E}_{b\sim p(b)}[\log(1-D_S(G_S(E_B^c(b),z_b)))] \qquad (3)
$$

其中$D_S$试图最大化目标函数来区分去模糊的图片和真实清晰图片。，作为对比，$G_S$的目标是最小化loss使去模糊的图片与真实图片域$S$中的真实样本看起来更相似。模糊后图片域的对抗损失定义为：
$$
\mathcal{L}_{D_B}=\mathbb{E}_{b\sim p(b)}[\log D_B(b)]+\mathbb{E}_{s\sim p(s)}[log(1-D_B(G_B(E_S^c(s),z_b)))] \qquad (4)
$$

### 3.3. Cycle-Consistency Loss

在最值游戏中与判别器$D_S$竞争过后，$G_S$应该能够生成视觉上真实的清晰图片。但是由于没有配对的监督存在，去模糊的图像也许无法保留原始模糊图像中的内容信息。受CycleGAN启发，引入循环一致性损失用于保证去模糊图片$s_b$能够被模糊重构成原始模糊样本，且$b_s$也可以转换回原始清晰图片域。循环一致性损失进一步限制了生成样本空间，并保留了原始图片的内容。具体来说，前向变换为：
$$
s_b=G_S(E_B^c(b), E^b(b)), b_s=G_B(E_S^c(s),E^b(b)) \qquad(5)
$$
反向变换为：
$$
\hat{b}=G_B(E_S^c(s_b),E^b(b_s)),\hat{s}=G_S(E_B^c(b_s),E^b(b_s)) \qquad(6)
$$
两个域上的循环一致性损失定义为：
$$
\mathcal{L}_{cc}=\mathbb{E}_{s\sim p(s)}[\Vert s-\hat{s}\Vert_1]+\mathbb{E}_{b\sim p(b)}[\Vert b-\hat{b}\Vert_1] \qquad (7)
$$

### 3.4. Perceptual Loss

从之前的实验中发现，生成的去模糊样本通常包含很多讨厌的人工痕迹。受启发，一个预训练好的深度网络提取的特征包含大量语义信息，且可以把它们的距离当作感知相似度评价，因此在去模糊图片和对应的原始模糊图片间加入一个感知loss：
$$
\mathcal{L}_p=\Vert \phi_l(s_b)-\phi_l(b)\Vert_2^2 \qquad(8)
$$
其中$\phi_l(x)$是预训练的CNN第$l$层的特征。我们使用在ImageNet上预训练的VGG-19网络的3*3卷积层。

使用模糊图片$b$而非清晰图片$s$作为感知loss的参考图片的主要原因有两个。第一，假设$b$的内容信息能被预训练的CNN提取。如Figure 4.2所示，实验结果验证了这一观点。第二，由于$s$和$b$是非配对的，如果在$s$和$s_b$之间使用感知loss会强制$s_b$编码$s$中不相关的内容信息。但是因为我们同样注意到感知loss对模糊敏感，所以小心地平衡感知loss和其他loss的权重以防止$s_b$和$b$太靠近。

需要提到的是，并没有在$b_s$和$s$上使用感知loss. 这是因为在训练过程中我们并没有发现$b_s$中有明显的人工痕迹。而且，对于文字图像去模糊，因为观察到感知loss对性能表现不仅没有提升反而会降低，因此也没有引入。一个可能的原因也许是因为文字图像的像素密度分布和ImageNet中自然图像的像素密度分布的差异很大。

完整的目标函数是所有loss的加权和：
$$
\mathcal{L}=\lambda_{adv}\mathcal{L}_{adv}+\lambda_{KL}\mathcal{L}_{KL}+\lambda_{cc}\mathcal{L}_{cc}+\lambda_{p}\mathcal{L}_{p} \qquad(9)
$$
其中$\mathcal{L}_{adv}=\mathcal{L}_{D_S}+\mathcal{L}_{D_B}$

### 3.5. Testing

测试的时候，去掉了模糊分支。给定一张测试的模糊图像$b_t$, $E_B^c$和$E^b$分别提取内容和模糊特征。然后$G_S$接收输出并生成去模糊后的图像$S_{b_t}$:
$$
s_{b_t}=G_S(E_B^c(b_t),E^b(b_t))
$$

### 3.6. Implementation Details

**Architecture and training details.** 使用与`<Diverse image-to-image translation via disentangled representations>`相似的网络架构。内容编码器由3个卷积层和4个残差块构成。模糊编码器包含4个卷积层和一个全连接层。生成器的架构内容编码器是对称的，四个残差块后接三个转置卷积层。判别器使用一个多尺度结构，每个尺度的特征图经过5个卷积层和sigmoid输出。训练时使用Adam优化器，前40个epochs的学习率为0.0002，然后接下来40个epochs使用指数衰减。随机剪裁$128\times128$图像，batch size为16.

$\lambda_{adv}=1,\lambda_{KL}=0.01,\lambda_{cc}=10,\lambda{p}=0.1$

**Motion blur generation. **采用DeblurGAN的方式生成动态模糊核为图像加模糊，轨迹是随机生成的。然后通过对轨迹向量使用点插值法（ sub-pixel interpolation）生成核。



## 4. Experimental Results

### 4.1. Datasets and Metrics

